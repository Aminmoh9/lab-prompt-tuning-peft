{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oCA0sm55VaS-",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Lab | Introduction to Prompt Tuning using PEFT from Hugging Face\n",
    "\n",
    "<!-- ### Fine-tune a Foundational Model effortless -->\n",
    "\n",
    "**Note:** This is more or less the same notebook you saw in the previous lesson, but that is ok. This is an LLM fine-tuning lab. In class we used a set of datasets and models, and in the labs you are required to change the LLMs models and the datasets including the pre-processing pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6fba2d42-ed99-4a03-8033-d479ce24d5dd",
     "showTitle": false,
     "title": ""
    },
    "id": "2vkOvTEsVaTA"
   },
   "source": [
    "# Prompt Tuning\n",
    "\n",
    "## Brief introduction to Prompt Tuning.\n",
    "It’s an Additive Fine-Tuning technique for models. This means that we WILL NOT MODIFY ANY WEIGHTS OF THE ORIGINAL MODEL. You might be wondering, how are we going to perform fine-tuning then? Well, we will train additional layers that are added to the model. That’s why it’s called an Additive technique.\n",
    "\n",
    "Considering it’s an Additive technique and its name is Prompt-Tuning, it seems clear that the layers we’re going to add and train are related to the prompt.\n",
    "\n",
    "![My Image](https://github.com/peremartra/Large-Language-Model-Notebooks-Course/blob/main/img/Martra_Figure_5_Prompt_Tuning.jpg?raw=true)\n",
    "\n",
    "We are creating a type of superprompt by enabling a model to enhance a portion of the prompt with its acquired knowledge. However, that particular section of the prompt cannot be translated into natural language. **It's as if we've mastered expressing ourselves in embeddings and generating highly effective prompts.**\n",
    "\n",
    "In each training cycle, the only weights that can be modified to minimize the loss function are those integrated into the prompt.\n",
    "\n",
    "The primary consequence of this technique is that the number of parameters to train is genuinely small. However, we encounter a second, perhaps more significant consequence, namely that, **since we do not modify the weights of the pretrained model, it does not alter its behavior or forget any information it has previously learned.**\n",
    "\n",
    "The training is faster and more cost-effective. Moreover, we can train various models, and during inference time, we only need to load one foundational model along with the new smaller trained models because the weights of the original model have not been altered\n",
    "\n",
    "## What are we going to do in the notebook?\n",
    "We are going to train two different models using two datasets, each with just one pre-trained model from the Bloom family. One will be trained to generate prompts and the other to detect hate in sentences.\n",
    "\n",
    "Additionally, we'll explore how to load both models with only one copy of the foundational model in memory.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tZhdbTh-VaTA"
   },
   "source": [
    "## Loading the Peft Library\n",
    "This library contains the Hugging Face implementation of various fine-tuning techniques, including Prompt Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d16bf5ec-888b-4c76-a655-193fd4cc8a36",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JechhJhhVaTA",
    "outputId": "6387c59e-9868-4bcb-a627-4ba6cf213e1b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m125.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m110.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.1/199.1 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.1/316.1 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.6/302.6 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -q transformers==4.41.2\n",
    "!pip install -q peft==0.10.0\n",
    "!pip install -q datasets==2.20.0\n",
    "!pip install -q accelerate==0.30.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GGbh426RVaTB"
   },
   "source": [
    "From the transformers library, we import the necessary classes to instantiate the model and the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "31738463-c9b0-431d-869e-1735e1e2f5c7",
     "showTitle": false,
     "title": ""
    },
    "id": "KWOEt-yOVaTB"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6qYsnwjSVaTC"
   },
   "source": [
    "## Loading the model and the tokenizers.\n",
    "\n",
    "Bloom is one of the smallest and smartest models available for training with the PEFT Library using Prompt Tuning.\n",
    "\n",
    "I'm opting for the smallest one to minimize training time and avoid memory issues in Colab. Feel Free to try with a bigger one if you have acces to a good GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "MnqIhv2UVaTC"
   },
   "outputs": [],
   "source": [
    "model_name = \"bigscience/bloomz-560m\"\n",
    "NUM_VIRTUAL_TOKENS = 20\n",
    "#If you just want to test the solution, you can reduce the EPOCHs.\n",
    "NUM_EPOCHS_PROMPT = 5\n",
    "NUM_EPOCHS_CLASSIFIER = 5\n",
    "device = \"cuda\" #Replace with \"mps\" for Silicon chips."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 301,
     "referenced_widgets": [
      "e75660082dfa4f7fb21c397a25410925",
      "762bd39960904694848a330c4f9c2515",
      "26e3194f4ad54bdbb21e22531c9f89c2",
      "705f8d06201140388a5249acac3e7938",
      "594c74dbc006426dbf9087d5b12ea0a0",
      "500a61db3b734a2ebf8097c528b9e3a9",
      "438b2c7f21494e0fbfe2faa627a49e71",
      "2d61921a9ae84ca98b03262c080c8a0c",
      "3f89749bdb524ef798e78229471ba81f",
      "f29a59917b794e7ea0f7a0339eecd97a",
      "2be9d4f92fbf493bb2b0811e9002b004",
      "f9d68e2f50ff4e8ba238cef71af68b90",
      "fcc83e6b83084a5f8c8b7c56bfe5aed5",
      "f1fbdb17a54546f9a78f3caaf54a9afe",
      "f765efb401674c33b9af7df44f614c90",
      "bb0ad36ce1a64cc0aade7ff48c04dcbc",
      "336f0dcedb5943e1a180226073965f94",
      "4d7568fd6d8d4b66bc61bca3729ded3a",
      "cfa19d888ebf44229e99b1dcebcbc812",
      "d37a0e530d5f47d9be19038184ee61cb",
      "25ab97a6cf88436a9a96c075f78c5a1a",
      "e9dd0bf216b84660acefed02db91e6a0",
      "a04333100658471a87cd7d07c04b33a7",
      "392ede50d1024ed5826c809f1c18c5ae",
      "3e47846b92e24d70b5c34c663f11bf2a",
      "73563dbd0d7e4e2eb45e5b3a93215b84",
      "4a34e7f550f7442bad5bec84032c54ea",
      "b2f9c5f8285e40299c1e55cca85ede56",
      "184979911f364576ab578b94707d8be7",
      "bfee534cc0ad4b12a7ec9bcf36f3ef92",
      "d2b121754a364842adc6e88d74e97474",
      "8330cf647bb1419182680f4a28315574",
      "b0a06ceb4ce347d9b76612882ab66edb",
      "b8ba71a98b3f4eb49b90e1465c97602c",
      "d7b1b6451c134ea4ac36cee5deb6f6a9",
      "b9d9b1fc0c7a4458b6d600017e616ecb",
      "f2e4fee09f7f44668999cc49a5bf1856",
      "a6b2db67c8544e728c0854cf917630cf",
      "b407422c25b34faca928f415dbfd0cd2",
      "603c40cfc18646bb913aea298eeec270",
      "0661821379f246b7af33f8789c10f482",
      "c24451fe842544099e46215820d94032",
      "8609940f0a514b699d10edbbdd99d952",
      "152e2b174b054a428e2ba70fea0e478d",
      "8fbd2fc1c8ce4392b67ecb2b92798077",
      "dc02349cd31f405d8f8b0017d2b421b4",
      "6bed9c37dfa04b929ae89a8e8dc453e6",
      "1c056f21d32b40ffa8b6f05fa2e9c165",
      "7a5b49fa18f943e79eecdee51576185a",
      "fe9acfa6df0349878af238ae68847a4a",
      "68a0aed56cfb499884b0c24fc7c17204",
      "a49fa537597a4d5dab701cec53abd38b",
      "6b57decf579e404ab91cf66d2af807c4",
      "3d07b82106164e48834ad9834b9acbbc",
      "0ebe7e058f634b04aa9f8b332e4eb492"
     ]
    },
    "id": "fSMu3qRsVaTC",
    "outputId": "65401445-ece0-4be6-ebae-003763f0ac31"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e75660082dfa4f7fb21c397a25410925",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/222 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9d68e2f50ff4e8ba238cef71af68b90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/14.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a04333100658471a87cd7d07c04b33a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/85.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8ba71a98b3f4eb49b90e1465c97602c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/715 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fbd2fc1c8ce4392b67ecb2b92798077",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "foundational_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    device_map = device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8W2fWhOnVaTC"
   },
   "source": [
    "## Inference with the pre trained bloom model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "47j2D3WWVaTC"
   },
   "outputs": [],
   "source": [
    "#this function returns the outputs from the model received, and inputs.\n",
    "def get_outputs(model, inputs, max_new_tokens=100): #PLAY WITH THIS FUNCTION AS YOU SEE FIT\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        #temperature=0.2,\n",
    "        #top_p=0.95,\n",
    "        #do_sample=True,\n",
    "        repetition_penalty=1.5, #Avoid repetition.\n",
    "        early_stopping=True, #The model can stop before reach the max_length\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ca4d203a-5152-4947-ab34-cfd0b40a102a",
     "showTitle": false,
     "title": ""
    },
    "id": "kRLSfuo2VaTC"
   },
   "source": [
    "To compare the pre-trained model with the same model after the prompt-tuning process, I will run the same sentence on both models.\n",
    "\n",
    "Since I'm creating a model that can generate prompts, I'll instruct it to provide a prompt that makes it act like a fitness trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1d4c80a9-4edd-4fcd-aef0-996f4da5cc02",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QvStaT7cVaTC",
    "outputId": "14022ff4-6101-410a-aa30-32698892f9cc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:563: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Act as a fitness Trainer. Prompt:']\n"
     ]
    }
   ],
   "source": [
    "input_prompt = tokenizer(\"Act as a fitness Trainer. Prompt:\", return_tensors=\"pt\")\n",
    "foundational_outputs_prompt = get_outputs(foundational_model,\n",
    "                                          input_prompt.to(device),\n",
    "                                          max_new_tokens=50)\n",
    "\n",
    "print(tokenizer.batch_decode(foundational_outputs_prompt, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5d-PY1yAh2mB"
   },
   "source": [
    "The model doesn't know what its mission is and answers as best as it can. It's not a bad response, but it's not what we're looking for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f438d43b-6b9f-445e-9df4-60ea09640764",
     "showTitle": false,
     "title": ""
    },
    "id": "OGbJTbRnVaTD"
   },
   "source": [
    "# Prompt Creator\n",
    "## Preparing Datasets\n",
    "The Dataset used, for this first example, is:\n",
    "* https://huggingface.co/datasets/fka/awesome-chatgpt-prompts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "RD8H_LLaVaTD"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2ed62b41-e3fa-4a41-a0a9-59f35a6904f9",
     "showTitle": false,
     "title": ""
    },
    "id": "xmAp_o4PVaTD"
   },
   "outputs": [],
   "source": [
    "dataset_prompt = \"fka/awesome-chatgpt-prompts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "qaU4FmgwAzZK"
   },
   "outputs": [],
   "source": [
    "def concatenate_columns_prompt(dataset):\n",
    "    def concatenate(example):\n",
    "        example['prompt'] = \"Act as a {}. Prompt: {}\".format(example['act'], example['prompt'])\n",
    "        return example\n",
    "\n",
    "    dataset = dataset.map(concatenate)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 177,
     "referenced_widgets": [
      "0ba062ef7d3944b39e25807a6ddc517b",
      "c214196353a248c0ba5b2225f020315e",
      "be0ec0402dce4e3b9090d2248256d30b",
      "bcd5484b2869402899a88635f4ab31ff",
      "d24d7e3d86114ac6b58862feb823626f",
      "65f9b83b5be0474b946cceecf2162169",
      "448d21740a864a898d17d6455765d6db",
      "8f10349f9a37425eb3e4eac9655397b2",
      "5f1291775e474cb693aa92cf9b0f5236",
      "f949fa07b2d6450aa7b076f4f2281e00",
      "a166598d57624ce392fb15890c81ae99",
      "fe48921fc7084f5cbf148ed466e5a65c",
      "1513534d5be8481ea3e99ff7e24a38f6",
      "52f0499f55624ee4a3b3dd62adcfefca",
      "d10b29d8f6a44f9daae0f14381fd9946",
      "0fcfab3cd8d34c3e87958efd3b91d590",
      "11af67d28fe34ed9b2e57ba6c0f4bdf6",
      "968c48f25fd640b69e4f4ab6bd945bfc",
      "93a5d88bb6be4e6f9d63094a23fcae4b",
      "787fd76d5c9b4ae68607f9ee9e60b904",
      "7708245dda704b798a2826faf6f8887b",
      "b0b484cfaadc41a6aca09ac886bb1af9",
      "642f4889652e4ca1a2079e1447d58e18",
      "916361102ff148559f715908f66cc051",
      "f6f8024e3a004fb4a9183edce3345581",
      "4c393434b74e477791f02e7e9841809f",
      "5f2bd76301a74961a9bfe3dc48ede4a4",
      "0307a4335a5f491e95c6ff07d0aff0d6",
      "2fbbaa4dbbaa4129a12711a86d6eb7cb",
      "03b69694797c41a1896974977170ab7c",
      "d8b5e485f39d4b19a4280995d7a39c17",
      "03492536b0d14243b8ad8d562dbd5a52",
      "582fee51ec8f4a87afd8760cdc649900",
      "44714d09529a4855b35d9aa98b82da95",
      "159ecad6ba9647edabe92f94054404ef",
      "38382c2bdd5b430684eb5417462d6527",
      "cc38a2ac6e6841e2b5eb29a3e5dee1b2",
      "51974b0124e0426aa18bd620e1a1e26d",
      "86fd64baa86e4bd6ac54522196f7a813",
      "72ca8268573c45f7ae993a63072d7df7",
      "027444fc5be7444392afbb65b6b873eb",
      "abebcd78c7dc49d5875f43a3fb0a13e3",
      "c1e363c00d654c74afc56db70db833ab",
      "a5b9516f560b4c1696770c93d05d67e4",
      "3c1b690b40f740f9b6821572dbc1ba81",
      "40327eca2d2649d097001c0c0c89a13c",
      "497411dbfa5d4ce8be05eeab283e0f02",
      "fb3b3fd6be3f41ce8106a03f17436ab0",
      "18e3e03fad4d4f42b890cdb5161c97be",
      "0bd99868c20443509f8ea175c1fcec0c",
      "3b2abda9473944f38f329a6848a5ef59",
      "d9f07b823365410cbf550114f995ffa7",
      "886806787cc74d0ea6cd77001ab0bc4c",
      "515199ec467146d792c183db4a20e870",
      "4b445e07f09e427ba79243cc6c73b54b"
     ]
    },
    "id": "uoL6qitsLo0o",
    "outputId": "d75beca3-66da-43db-9327-aa5d65cdd4b1"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ba062ef7d3944b39e25807a6ddc517b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/339 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe48921fc7084f5cbf148ed466e5a65c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/104k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "642f4889652e4ca1a2079e1447d58e18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/203 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44714d09529a4855b35d9aa98b82da95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/203 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c1b690b40f740f9b6821572dbc1ba81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/203 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Create the Dataset to create prompts.\n",
    "data_prompt = load_dataset(dataset_prompt)\n",
    "data_prompt['train'] = concatenate_columns_prompt(data_prompt['train'])\n",
    "\n",
    "data_prompt = data_prompt.map(lambda samples: tokenizer(samples[\"prompt\"]), batched=True)\n",
    "train_sample_prompt = data_prompt[\"train\"].remove_columns('act')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QL1CwC9tGGSn",
    "outputId": "6cd42742-bab4-4c99-93c1-5b9ea78da174"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['prompt', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 203\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(train_sample_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rzA3rLTk8XW8",
    "outputId": "78e4be6d-0a7d-4905-f97f-d76ea52e13ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': ['Act as a An Ethereum Developer. Prompt: Imagine you are an experienced Ethereum developer tasked with creating a smart contract for a blockchain messenger. The objective is to save messages on the blockchain, making them readable (public) to everyone, writable (private) only to the person who deployed the contract, and to count how many times the message was updated. Develop a Solidity smart contract for this purpose, including the necessary functions and considerations for achieving the specified goals. Please provide the code and any relevant explanations to ensure a clear understanding of the implementation.', \"Act as a SEO Prompt. Prompt: Using WebPilot, create an outline for an article that will be 2,000 words on the keyword 'Best SEO prompts' based on the top 10 results from Google. Include every relevant heading possible. Keep the keyword density of the headings high. For each section of the outline, include the word count. Include FAQs section in the outline too, based on people also ask section from Google for the keyword. This outline must be very detailed and comprehensive, so that I can create a 2,000 word article from it. Generate a long list of LSI and NLP keywords related to my keyword. Also include any other words related to the keyword. Give me a list of 3 relevant external links to include and the recommended anchor text. Make sure they’re not competing articles. Split the outline into part 1 and part 2.\"], 'input_ids': [[8972, 661, 267, 2246, 28857, 167625, 170786, 17, 36949, 1309, 29, 156301, 1152, 1306, 660, 72560, 28857, 167625, 84544, 20165, 376, 1002, 26168, 267, 30479, 17477, 613, 267, 120755, 238776, 17, 1387, 47881, 632, 427, 14565, 29866, 664, 368, 120755, 15, 16997, 4054, 136044, 375, 4859, 12, 427, 39839, 15, 9697, 1242, 375, 13614, 12, 3804, 427, 368, 2298, 5268, 109891, 368, 17477, 15, 530, 427, 11210, 4143, 7112, 11866, 368, 11011, 1620, 36320, 17, 21265, 267, 11550, 90533, 30479, 17477, 613, 1119, 27343, 15, 11762, 368, 18348, 16231, 530, 127246, 613, 94510, 368, 25605, 55790, 17, 29901, 13842, 368, 4400, 530, 2914, 24466, 184637, 427, 22646, 267, 11285, 32391, 461, 368, 17786, 17], [8972, 661, 267, 174249, 36949, 1309, 17, 36949, 1309, 29, 43252, 15202, 51, 46712, 15, 7932, 660, 67606, 613, 660, 8723, 861, 2152, 722, 415, 15, 1874, 17848, 664, 368, 70062, 38038, 388, 174249, 39841, 9427, 11173, 664, 368, 7921, 1581, 9649, 1485, 8943, 17, 137151, 6216, 24466, 87480, 6399, 17, 109988, 368, 70062, 32993, 461, 368, 10082, 3164, 6426, 17, 5070, 5546, 13773, 461, 368, 67606, 15, 13756, 368, 14679, 11210, 17, 137151, 209147, 86, 13773, 361, 368, 67606, 10136, 15, 11173, 664, 6199, 3466, 9283, 13773, 1485, 8943, 613, 368, 70062, 17, 3904, 67606, 6591, 722, 5636, 53180, 530, 65604, 15, 1427, 861, 473, 1400, 7932, 267, 415, 15, 1874, 14679, 8723, 1485, 718, 17, 143293, 267, 3829, 4737, 461, 499, 49863, 530, 557, 103096, 135158, 17251, 427, 2670, 70062, 17, 30497, 13756, 2914, 3390, 17848, 17251, 427, 368, 70062, 17, 121045, 1074, 267, 4737, 461, 735, 24466, 26331, 31437, 427, 13756, 530, 368, 49635, 91770, 5484, 17, 42187, 11097, 3291, 87099, 1130, 174169, 14476, 17, 152830, 368, 67606, 3727, 1571, 404, 530, 1571, 415, 17]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "print(train_sample_prompt[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b97381d4-5fe2-49d0-be5d-2fe3421edc5c",
     "showTitle": false,
     "title": ""
    },
    "id": "0-5mv1ZpVaTD"
   },
   "source": [
    "## prompt-tuning configuration.  \n",
    "\n",
    "API docs:\n",
    "https://huggingface.co/docs/peft/main/en/package_reference/tuners#peft.PromptTuningConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6df8e1f1-be9e-42db-b4a4-6af7cd351004",
     "showTitle": false,
     "title": ""
    },
    "id": "sOg1Yh-oVaTD"
   },
   "outputs": [],
   "source": [
    "from peft import  get_peft_model, PromptTuningConfig, TaskType, PromptTuningInit\n",
    "\n",
    "generation_config_prompt = PromptTuningConfig( #PLAY WITH THIS CONFIG IF YOU LIKE\n",
    "    task_type=TaskType.CAUSAL_LM, #This type indicates the model will generate text.\n",
    "    prompt_tuning_init=PromptTuningInit.RANDOM,  #The added virtual tokens are initializad with random numbers\n",
    "    num_virtual_tokens=NUM_VIRTUAL_TOKENS, #Number of virtual tokens to be added and trained.\n",
    "    tokenizer_name_or_path=model_name #The pre-trained model.\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "an9KBtB1VaTD"
   },
   "source": [
    "We will create two  prompt tuning models using the same pre-trained model and the same config, but with a different Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c_D8oDQZVaTD",
    "outputId": "ea54c51a-29a5-4710-cf14-9c8224e8d0aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 20,480 || all params: 559,235,072 || trainable%: 0.0036621451381361144\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "peft_model_prompt = get_peft_model(foundational_model, generation_config_prompt)\n",
    "print(peft_model_prompt.print_trainable_parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cff5bc33-8cfb-4144-8962-9c54362a7faa",
     "showTitle": false,
     "title": ""
    },
    "id": "i6WhJSUwVaTE"
   },
   "source": [
    "**That's amazing: did you see the reduction in trainable parameters? We are going to train a 0.001% of the paramaters available.**\n",
    "\n",
    "Now we are going to create the training arguments, and we will use the same configuration in both trainings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "SJoznfzjVaTE"
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "def create_training_arguments(path, learning_rate=0.0035, epochs=6, autobatch=True):\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=path, # Where the model predictions and checkpoints will be written\n",
    "        #use_cpu=True, # This is necessary for CPU clusters.\n",
    "        auto_find_batch_size=autobatch, # Find a suitable batch size that will fit into memory automatically\n",
    "        learning_rate= learning_rate, # Higher learning rate than full fine-tuning\n",
    "        #per_device_train_batch_size=4,\n",
    "        num_train_epochs=epochs,\n",
    "        report_to=\"none\"  # Disable wandb and other logging integrations\n",
    "    )\n",
    "    return training_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "54b78a8f-81f0-44c0-b0bc-dcb14891715f",
     "showTitle": false,
     "title": ""
    },
    "id": "cb1j50DSVaTE"
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "working_dir = \"./\"\n",
    "\n",
    "#Is best to store the models in separate folders.\n",
    "#Create the name of the directories where to store the models.\n",
    "output_directory_prompt =  os.path.join(working_dir, \"peft_outputs_prompt\")\n",
    "output_directory_classifier =  os.path.join(working_dir, \"peft_outputs_classifier\")\n",
    "\n",
    "#Just creating the directoris if not exist.\n",
    "if not os.path.exists(working_dir):\n",
    "    os.mkdir(working_dir)\n",
    "if not os.path.exists(output_directory_prompt):\n",
    "    os.mkdir(output_directory_prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OC5IhO9mVaTE"
   },
   "source": [
    "We need to indicate the directory containing the model when creating the TrainingArguments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c593deb6-5626-4fd9-89c2-2329e2f9b6e0",
     "showTitle": false,
     "title": ""
    },
    "id": "GdMfjk5RVaTE"
   },
   "source": [
    "## Training first model\n",
    "\n",
    "We will create the trainer Object, one for each model to train.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "D4v4RSSeVaTE"
   },
   "outputs": [],
   "source": [
    "training_args_prompt = create_training_arguments(output_directory_prompt,\n",
    "                                                 3e-2,\n",
    "                                                 NUM_EPOCHS_PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "uVAfNdEIVaTE"
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer, DataCollatorForLanguageModeling\n",
    "def create_trainer(model, training_args, train_dataset):\n",
    "    trainer = Trainer(\n",
    "        model=model, # We pass in the PEFT version of the foundation model, bloomz-560M\n",
    "        args=training_args, #The args for the training.\n",
    "        train_dataset=train_dataset, #The dataset used to train the model.\n",
    "        data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False) # mlm=False indicates not to use masked language modeling\n",
    "    )\n",
    "    return trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "32e43bcf-23b2-46aa-9cf0-455b83ef4f38",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 168
    },
    "id": "1Sz9BeFZVaTF",
    "outputId": "eb092ff7-25f3-489e-aa61-c7c46acce3e3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15' max='130' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 15/130 00:16 < 02:29, 0.77 it/s, Epoch 0.54/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='255' max='255' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [255/255 02:22, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=255, training_loss=2.8081964231004903, metrics={'train_runtime': 142.83, 'train_samples_per_second': 7.106, 'train_steps_per_second': 1.785, 'total_flos': 324586719363072.0, 'train_loss': 2.8081964231004903, 'epoch': 5.0})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training first model.\n",
    "trainer_prompt = create_trainer(peft_model_prompt,\n",
    "                                training_args_prompt,\n",
    "                                train_sample_prompt)\n",
    "trainer_prompt.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Veg8ziHvWh4I"
   },
   "source": [
    "Release GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uYVcJDSP7Sq-",
    "outputId": "d404a096-2266-4da6-ce1a-36c981ed3c5e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5a6c8daf-8248-458a-9f6f-14865b4fbd2e",
     "showTitle": false,
     "title": ""
    },
    "id": "s5k10HwoVaTG"
   },
   "source": [
    "## Save model\n",
    "We are going to save the model. These models are ready to be used, as long as we have the pre-trained model from which they were created in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "409df5ce-e496-46d7-be2c-202a463cdc80",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E3dn3PeMVaTG",
    "outputId": "643293d3-8b1e-4814-af1a-7a47fd2f5f9e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer_prompt.model.save_pretrained(output_directory_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fb14e3fd-bbf6-4d56-92c2-51bfe08de72a",
     "showTitle": false,
     "title": ""
    },
    "id": "rkUKpDDWVaTG"
   },
   "source": [
    "## Inference first tuned model\n",
    "\n",
    "You can load the model from the path that you have saved to before, and ask the model to generate text based on our input before!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cc48af16-c117-4019-a31a-ce1c93cd21d4",
     "showTitle": false,
     "title": ""
    },
    "id": "dlqXXN8oVaTG"
   },
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "loaded_model_peft = PeftModel.from_pretrained(foundational_model,\n",
    "                                         output_directory_prompt,\n",
    "                                         #device_map=device,\n",
    "                                         is_trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jjXT-6EKMiXk",
    "outputId": "fa21fb19-ca73-40a0-c906-3ba184d1afb0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:563: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Act as a fitness Trainer. Prompt: I want you to act like an expert in your field of expertise and provide me with the best advice for my clients on how they can improve their performance, increase confidence or even lose weight.  My first request is \"I need help improving personal health\" ']\n"
     ]
    }
   ],
   "source": [
    "loaded_model_prompt_outputs = get_outputs(loaded_model_peft,\n",
    "                                          input_prompt,\n",
    "                                          max_new_tokens=50)\n",
    "print(tokenizer.batch_decode(loaded_model_prompt_outputs, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MzjDgDE2reTO"
   },
   "source": [
    "Let's compare the result of the model before and after being fine-tuned with prompt-tuning.\n",
    "\n",
    "**Input for the model**\n",
    "```\n",
    "Act as a fitness Trainer. Prompt:\n",
    "```\n",
    "\n",
    "**Original model**\n",
    "```\n",
    "Act as a fitness Trainer. Prompt:  Follow up with your trainer\n",
    "```\n",
    "**Trained for classification with Prompt-tuning** 50 Epochs:\n",
    "```\n",
    "Act as a fitness Trainer. Prompt: ＋ Acts like an expert in the field of sports and health, but does not provide detailed information about his work or products to help you understand them better.  + I want my first client referred me through this website for their gym membership program which is based on physical activity training exercises that are easy enough (eight minutes) per week with no need any special equipment required.   - First Question : What would be your role?\n",
    "```\n",
    "\n",
    "It's very clear that the result is quite different, it's not exactly what we're looking for but it's much closer.\n",
    "\n",
    "It's possible that we're at the limit of what Bloom's smallest model can offer. Try with any other model, surely with the one with 1B parameters the result will be better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hVif-42UCP7l"
   },
   "source": [
    "# Hate Classifier\n",
    "##Loading the Dataset\n",
    "\n",
    "* https://huggingface.co/datasets/SetFit/ethos_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Pyp64F0tRQBt",
    "outputId": "8069821e-885b-4252-e6ed-4ffff4e888b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Sentence : I don't like short people, no idea why they exist. Label : No\"]\n"
     ]
    }
   ],
   "source": [
    "input_classifier = tokenizer(\"Sentence : I don't like short people, no idea why they exist. Label :\", return_tensors=\"pt\")\n",
    "foundational_outputs_prompt = get_outputs(foundational_model,\n",
    "                                          input_classifier.to(device),\n",
    "                                          max_new_tokens=50)\n",
    "\n",
    "print(tokenizer.batch_decode(foundational_outputs_prompt, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N1gk7C_NSt-Y"
   },
   "source": [
    "The model has no idea what its purpose is, so it completes the sentence as best as it can."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "wlLAsUvhE09L"
   },
   "outputs": [],
   "source": [
    "dataset_classifier = \"SetFit/ethos_binary\"\n",
    "\n",
    "def concatenate_columns_classifier(dataset):\n",
    "    def concatenate(example):\n",
    "        example['text'] = \"Sentence : {} Label : {}\".format(example['text'], example['label_text'])\n",
    "        return example\n",
    "\n",
    "    dataset = dataset.map(concatenate)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 308,
     "referenced_widgets": [
      "8314520b977d4f108be8c07b2e6f0e87",
      "663589fb80b840589a88d5cf352ce88a",
      "601e56dd0eb4422c863663b0f84b0730",
      "801ac8a5e8a947f7a4d4996aa9c59e39",
      "eb1c7ed61ffd4f82a19f7bedeed8eea9",
      "5e9f654102c5477798f44571096c63f9",
      "9eeb303ae46d4e8bb4e7ba0925854967",
      "520d9adfd7ee4a879ab19c0115df6b1a",
      "2de53472bb49424389719bddb36e5fad",
      "ffd38980117f402380d3c444c27049c9",
      "4ec64edadaf84370a299a26d41cfdda0",
      "3883bc20237046e7a943150e451f68da",
      "8c7f2544c6a647bd89f277b834362804",
      "e90141f3298e4d8ea289ac9d65892db1",
      "b43872f779b24188ab9b7cdf2e65b1aa",
      "91fd385681fa4154810fbe33a202ee23",
      "431cdc0432c04d2596571a91ba4c2f8c",
      "af943096fccf44d7899b749648b6c478",
      "b41efb11076848b7b3c3145c463f7d58",
      "47c4f3dab4564948a4436e5e68252a12",
      "58d94cb1d5b64f0598245e58168de555",
      "f8a661209d914e5c92fca4b94bcd5170",
      "725c8ed43a914b2fba66273646e4093b",
      "c875cba1c306453f800e6c707025db53",
      "edfac80595074cbba091b727ea88472e",
      "28d871f541fd4a26843511963c73c9ff",
      "d86bf074d6534248a0ae10499da342ab",
      "b75e0acc6a3c447d90998c65a314d303",
      "3e9664c83aa7430e80d2cbfdf731b06e",
      "50c9cec672944d01acf30a434cdb312b",
      "40cb09a9236f4cb4adf8c99d02deec86",
      "7784873518d24dc587c1c36081b1074c",
      "e807b3bf5a8840e6951022fe36830f52",
      "3fa7eb8f17c2481ab994dff57f247c18",
      "0601a674643348fc885d94ff38d7c632",
      "83ee12e4a5e148629c3a9195e67a7f38",
      "d2d18508a46f417fb79f63f06c5aeefc",
      "abe4273309d54a07b3d8a03bd5440d1a",
      "4b2ba161006944618391a2382cbc8b67",
      "fa0d17ad857b4656ab4a594fcfc441f4",
      "26827c6a73f3438ba11cdd5942f9d580",
      "cdaba57a974a4ac8a33e90c01ba9aacb",
      "6c8b2cb217f14ea590b788e76999fab7",
      "9bbb13e3c5e84bc28f95fef177084bd3",
      "052e593d879d4d0ba6aaf6813d752f33",
      "ae19b38919d2415a940a13941d24ec5a",
      "55057a0e287e46a18287220454088326",
      "7a84e71dd2044ef3af09f073a296fbe7",
      "e16e4256fac44bc4a4187d48813d9a9c",
      "34e9b0d98331484081e702d2e3d8f0c3",
      "e378fb2653214cf8a9792308d0ee4f62",
      "b37d2cda14f44ff3b3c68d092857e7b7",
      "47e6eecd594c4e358505eeb9bd1755c8",
      "1dfa73689c0d4e6dbe3a8d820ced66e0",
      "361d943e6ab2462781873f0978c97d77",
      "095d4957daba4b7f814fcdcc4d4cf95f",
      "61383f3686dd4b9b86998c6e43caeaee",
      "9cfe163bfc0845d8aa35747fc7a2468c",
      "10b8e3fc511642fa9f710f5a606ca4a2",
      "967c76bcdbe64ef4bffa3be1f8795b1b",
      "573447f0a7bd4c0a9ddf74a0c535833b",
      "acd7746b6db0429bb0860a98f362c563",
      "fa05d298d46a40cc82e5f5b664c0b96c",
      "4d6a6df11bdb462182b27398f8a85d9b",
      "292ea6333fe745d9bfa1a799c8d4c988",
      "5f1ca24e97df414887d42a9deedcdcfe",
      "3ab9e2599dc143778d878f934a139422",
      "95a69ce2819548ac849282bc7224ea50",
      "bcb4ebaf7430412e9ab1e552c730e73a",
      "59a3cd3882cd434aa22c0802e89cfe6e",
      "2007ec76061941fb9ed03ad0b6bb39c8",
      "5ffb88f25c924574a48794f75f75b104",
      "a3b4aa2b52d24513b500da876a64476e",
      "12568cdecdbd4bc19429e0ebc072cb42",
      "ce523a56fdd74f78b04df301861e5e14",
      "e1d210f0b93244d38153155d58666fe4",
      "870572dd70e945a183e791e0495f6da7",
      "664c8b6ee97141a5960be317de3442df",
      "96f97b4b7e9345008c97c6f8e2df6689",
      "672a05e86e4f47589eeef3e11a92bd4d",
      "f6fa89df3fa2465896f7a141ed8c1b4b",
      "d16388a514ac42beab3fd5b32a37570e",
      "a9af4ff3252b4c0a8ce0d92a656d7bc1",
      "af97fd071e8e48539da45af03aa0d43e",
      "3e06e50386014812a1dba6394a62ac53",
      "4905f7b72ccf42ee89e9b4fd41dd629c",
      "651ed53475ce45cba6534f94a9cc786b",
      "d0fb8e7340134e73a1135e3f2e7233b5"
     ]
    },
    "id": "5Y2K_3MqE4QV",
    "outputId": "bdcb7e89-dbb4-44b7-944c-15b0bbb65295"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8314520b977d4f108be8c07b2e6f0e87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/162 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "WARNING:huggingface_hub.repocard:Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3883bc20237046e7a943150e451f68da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/107k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "725c8ed43a914b2fba66273646e4093b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/64.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fa7eb8f17c2481ab994dff57f247c18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/598 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "052e593d879d4d0ba6aaf6813d752f33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "095d4957daba4b7f814fcdcc4d4cf95f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/598 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ab9e2599dc143778d878f934a139422",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/598 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "664c8b6ee97141a5960be317de3442df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_classifier = load_dataset(dataset_classifier)\n",
    "data_classifier['train'] = concatenate_columns_classifier(data_classifier['train'])\n",
    "\n",
    "data_classifier = data_classifier.map(lambda samples: tokenizer(samples[\"text\"]), batched=True)\n",
    "train_sample_classifier = data_classifier[\"train\"].remove_columns(['label', 'label_text', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ia-vz3ddTOY5",
    "outputId": "87832cf7-07e9-43ae-fe0d-9074a17d06c3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 'label_text', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 598\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label', 'label_text', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 400\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N8ROxvTEFAVC",
    "outputId": "6a04e02d-7747-410d-ccd0-7e338655f390"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask'],\n",
       "    num_rows: 598\n",
       "})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sample_classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XV2Z_LiRTMDC"
   },
   "source": [
    "I have deleted all the columns from the dataset that are not strictly necessary for training, that is to say, I have removed all columns that are not essential for the model's learning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SFUZNrAdFDR5",
    "outputId": "58d53a33-e7d0-4bb2-a269-7e78890ffa29"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[62121, 1671, 915, 473, 760, 10190, 513, 16154, 60, 19821, 138929, 20812, 426, 18833, 18816, 75536, 45617, 39469, 19368, 17956, 57274, 3758, 18065, 38, 44140, 17956, 72870, 8309, 9492, 15, 614, 156801, 85061, 48283, 44419, 426, 16472, 96789, 602, 45227, 43111, 181485, 435, 19821, 60, 48283, 44419, 426, 16472, 96789, 614, 156801, 77658, 915, 74549, 40423]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "print(train_sample_classifier[1:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q_F3oR70FF4z"
   },
   "source": [
    "## prompt-tuning configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "vr0Aw_byFL6n"
   },
   "outputs": [],
   "source": [
    "generation_config_classifier = PromptTuningConfig( #PLAY WITH THIS AS YOU SEE FIT\n",
    "    task_type=TaskType.CAUSAL_LM, #This type indicates the model will generate text.\n",
    "    prompt_tuning_init=PromptTuningInit.TEXT,  #\n",
    "    prompt_tuning_init_text=\"Indicates whether the sentence contains hate speech or not\",\n",
    "    num_virtual_tokens=NUM_VIRTUAL_TOKENS, #Number of virtual tokens to be added and trained.\n",
    "    tokenizer_name_or_path=model_name #The pre-trained model.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wWvoJMajFPcg",
    "outputId": "030b1aca-2b59-47cf-c94d-94acdf6b1bb5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 20,480 || all params: 559,235,072 || trainable%: 0.0036621451381361144\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "peft_model_classifier = get_peft_model(foundational_model, generation_config_classifier)\n",
    "print(peft_model_classifier.print_trainable_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "CpKtEudsFWTq"
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(output_directory_classifier):\n",
    "    os.mkdir(output_directory_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "DknsYEGwFW4g"
   },
   "outputs": [],
   "source": [
    "training_args_classifier = create_training_arguments(output_directory_classifier,\n",
    "                                                    3e-2,\n",
    "                                                    NUM_EPOCHS_CLASSIFIER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DAgEAjAMFasw"
   },
   "source": [
    "## Training Second Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 635
    },
    "id": "HfDLVy22FaNs",
    "outputId": "5b014757-c6fa-4ea7-b836-45f88dfd5e11"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='61' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 61/375 00:37 < 03:19, 1.57 it/s, Epoch 0.80/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='122' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [122/750 00:32 < 02:51, 3.67 it/s, Epoch 0.81/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='244' max='1495' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 244/1495 00:30 < 02:39, 7.84 it/s, Epoch 0.81/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2990' max='2990' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2990/2990 04:00, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.267300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>3.193700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>3.180300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>3.159300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>3.126400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2990, training_loss=3.1733291472878347, metrics={'train_runtime': 240.5633, 'train_samples_per_second': 12.429, 'train_steps_per_second': 12.429, 'total_flos': 360078925602816.0, 'train_loss': 3.1733291472878347, 'epoch': 5.0})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_classifier = create_trainer(peft_model_classifier,\n",
    "                                   training_args_classifier,\n",
    "                                   train_sample_classifier)\n",
    "trainer_classifier.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EcWL4-4OKCx9",
    "outputId": "b0db1777-1300-44f2-b229-7fbf55ced6bb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer_classifier.model.save_pretrained(output_directory_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FRcSaXMmM3mz"
   },
   "source": [
    "## Inference second Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "5Pkx0npHNWja"
   },
   "outputs": [],
   "source": [
    "loaded_model_peft.load_adapter(output_directory_classifier, adapter_name=\"classifier\")\n",
    "loaded_model_peft.set_adapter(\"classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "an8T4LvPJAUC",
    "outputId": "6b10f482-9902-4499-86a1-50aad341fb01"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:563: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Sentence : I don't like short people, no idea why they exist. Label : hate speech\"]\n"
     ]
    }
   ],
   "source": [
    "loaded_model_sentences_outputs = get_outputs(loaded_model_peft,\n",
    "                                             input_classifier, max_new_tokens=3)\n",
    "print(tokenizer.batch_decode(loaded_model_sentences_outputs, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SHbeFTXjVaTG"
   },
   "source": [
    "Let's check how the model's response has changed with training:\n",
    "\n",
    "**Input for the model**\n",
    "```\n",
    "Sentence : Head is the shape of a light bulb. Label :\n",
    "Sentence : I don't liky short people, no idea why they exist. Label :\n",
    "```\n",
    "\n",
    "**Original model**\n",
    "```\n",
    "Sentence : Head is the shape of a light bulb. Label :  head\n",
    "Sentence : I don't liky short people, no idea why they exist. Label :  No\n",
    "```\n",
    "**Trained for classification with Prompt-tuning**\n",
    "```\n",
    "Sentence : Head is the shape of a light bulb. Label :  no hate speech\n",
    "Sentence : I don't liky short people, no idea why they exist. Label :  hate speech\n",
    "```\n",
    "\n",
    "It's clear that the training has fulfilled its purpose. The original model doesn't know what its mission is and tries to complete the sentences as best as it can. On the other hand, the updated model with prompt-tuning does know what its mission is and is able to classify the sentences correctly and in the indicated format.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B6TUjNtGVaTH"
   },
   "source": [
    "# Exercise\n",
    "- Complete the prompts similar to what we did in class.\n",
    "     - Try at least 3 versions\n",
    "     - Be creative\n",
    " - Write a one page report summarizing your findings.\n",
    "     - Were there variations that didn't work well? i.e., where GPT either hallucinated or wrong\n",
    " - What did you learn?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lnmdVU4XLUIH"
   },
   "source": [
    "## Testing the Prompt Generator Model\n",
    "Let's test the trained prompt generator with different role variations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "lOAoEH6hLUIH"
   },
   "outputs": [],
   "source": [
    "# Load the prompt generator adapter\n",
    "loaded_model_peft.set_adapter(\"default\")  # Switch back to prompt generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HmlzcYXILUIH"
   },
   "source": [
    "### Test 1: Nutrition Expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SMJ6CdhwLUIH",
    "outputId": "68a2ddb4-4b96-41f9-dec4-fb08fb6d04e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1 - Nutrition Expert:\n",
      "['Act as a nutrition expert. Prompt: I want you to act like an experienced dietitian and explain how your body functions in order for me, who is not very knowledgeable about food or exercise but would be interested if someone could help with my research on the topic of weight loss. ']\n"
     ]
    }
   ],
   "source": [
    "test_input_1 = tokenizer(\"Act as a nutrition expert. Prompt:\", return_tensors=\"pt\")\n",
    "output_1 = get_outputs(loaded_model_peft, test_input_1.to(device), max_new_tokens=50)\n",
    "print(\"Test 1 - Nutrition Expert:\")\n",
    "print(tokenizer.batch_decode(output_1, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ddQUeKlELUIH"
   },
   "source": [
    "### Test 2: Motivational Speaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WBdahaCpLUIH",
    "outputId": "b8395c0e-90f4-4dda-da21-4df32e2bc7a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 2 - Motivational Speaker:\n",
      "['Act as a motivational speaker. Prompt: I want you to act like an expert in your field and explain the basics of that topic, then give examples from real life situations or anecdotes about it so people can learn more than just what they know already! My first request is \"I need some']\n"
     ]
    }
   ],
   "source": [
    "test_input_2 = tokenizer(\"Act as a motivational speaker. Prompt:\", return_tensors=\"pt\")\n",
    "output_2 = get_outputs(loaded_model_peft, test_input_2.to(device), max_new_tokens=50)\n",
    "print(\"Test 2 - Motivational Speaker:\")\n",
    "print(tokenizer.batch_decode(output_2, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AIrUPd-5LUIH"
   },
   "source": [
    "### Test 3: Career Counselor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FptlPlcqLUIH",
    "outputId": "6f455e08-e438-4946-85d0-92eb20d233a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 3 - Career Counselor:\n",
      "['Act as a career counselor. Prompt: I want you to act like an expert in your field and provide advice on how best approach the challenges of life, including financial decisions for yourself or others.  My first request is \"I need help with my finances\"  The second one is: \"Need some']\n"
     ]
    }
   ],
   "source": [
    "test_input_3 = tokenizer(\"Act as a career counselor. Prompt:\", return_tensors=\"pt\")\n",
    "output_3 = get_outputs(loaded_model_peft, test_input_3.to(device), max_new_tokens=50)\n",
    "print(\"Test 3 - Career Counselor:\")\n",
    "print(tokenizer.batch_decode(output_3, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ye_RcLEULUIH"
   },
   "source": [
    "## Testing the Hate Speech Classifier\n",
    "Let's test the hate speech classifier with different sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "21Ma-E-6LUII"
   },
   "outputs": [],
   "source": [
    "# Switch to classifier adapter\n",
    "loaded_model_peft.set_adapter(\"classifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RDXmsZIbLUII"
   },
   "source": [
    "### Test 4: Positive Statement (No Hate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "86ICp_y-LUII",
    "outputId": "4ac8c8ff-6af4-4acd-e124-aab2b574df43"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 4 - Positive Statement:\n",
      "['Sentence : Everyone deserves respect and kindness. Label : no hate speech']\n"
     ]
    }
   ],
   "source": [
    "test_classifier_1 = tokenizer(\"Sentence : Everyone deserves respect and kindness. Label :\", return_tensors=\"pt\")\n",
    "output_c1 = get_outputs(loaded_model_peft, test_classifier_1.to(device), max_new_tokens=5)\n",
    "print(\"Test 4 - Positive Statement:\")\n",
    "print(tokenizer.batch_decode(output_c1, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LFX6jO-nLUII"
   },
   "source": [
    "### Test 5: Borderline Negative Statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lRNSwRj6LUII",
    "outputId": "521d917d-1309-4fb4-a067-c972fa1be301"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 5 - Borderline Negative:\n",
      "['Sentence : That movie was terrible and a waste of time. Label : hate speech no']\n"
     ]
    }
   ],
   "source": [
    "test_classifier_2 = tokenizer(\"Sentence : That movie was terrible and a waste of time. Label :\", return_tensors=\"pt\")\n",
    "output_c2 = get_outputs(loaded_model_peft, test_classifier_2.to(device), max_new_tokens=5)\n",
    "print(\"Test 5 - Borderline Negative:\")\n",
    "print(tokenizer.batch_decode(output_c2, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fAh5ZXwPLUII"
   },
   "source": [
    "### Test 6: Clear Hate Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OLycHd3rLUII",
    "outputId": "c19e0842-0e7d-4cf5-e6ab-cb198254fbd9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 6 - Clear Hate Speech:\n",
      "['Sentence : People from that country are all criminals and should be banned. Label : hate speech no']\n"
     ]
    }
   ],
   "source": [
    "test_classifier_3 = tokenizer(\"Sentence : People from that country are all criminals and should be banned. Label :\", return_tensors=\"pt\")\n",
    "output_c3 = get_outputs(loaded_model_peft, test_classifier_3.to(device), max_new_tokens=5)\n",
    "print(\"Test 6 - Clear Hate Speech:\")\n",
    "print(tokenizer.batch_decode(output_c3, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Report\n",
    "\n",
    "## Test Results Summary\n",
    "\n",
    "I tested the models with 6 different variations:\n",
    "\n",
    "**Prompt Generator (3 tests):**\n",
    "- Nutrition Expert: Generated relevant prompts about diet and weight loss\n",
    "- Motivational Speaker: Created prompts about expertise and real-life examples\n",
    "- Career Counselor: Produced prompts focused on career and financial advice\n",
    "\n",
    "**Hate Speech Classifier (3 tests):**\n",
    "- Positive statement → Correctly identified as \"no hate speech\" ✅\n",
    "- Movie criticism → Incorrectly flagged, output was \"hate speech no\" ❌\n",
    "- Actual hate speech → Misclassified, output was \"hate speech no\" ❌\n",
    "\n",
    "## Variations That Didn't Work Well\n",
    "\n",
    "The hate speech classifier struggled with Tests 5 and 6, producing confusing outputs like \"hate speech no\" instead of clear labels. The model couldn't distinguish between negative sentiment and actual hate speech. Only 5 training epochs wasn't enough for this complex task.\n",
    "\n",
    "## What I Learned\n",
    "\n",
    "PEFT is highly efficient, training only 0.001% of parameters. Prompt generation worked well but hate speech classification struggled with the small model and limited training. Complex tasks need more epochs and larger models. Multiple adapters can load on one base model. PEFT is great for simple tasks but has limitations with nuanced classification."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "LLM 02 - Prompt Tuning with PEFT",
   "widgets": {}
  },
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
